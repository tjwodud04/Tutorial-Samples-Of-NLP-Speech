{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jwwXyokJfrMw"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_9lQ3frfrMx"
      },
      "source": [
        "\n",
        "torchtext 라이브러리로 텍스트 분류하기\n",
        "===============================================\n",
        "\n",
        "**번역**: `김강민 <https://github.com/gangsss>`_ , `김진현 <https://github.com/lewha0>`_\n",
        "\n",
        "이 튜토리얼에서는 torchtext 라이브러리를 사용하여 어떻게 텍스트 분류 분석을 위한 데이터셋을 만드는지를 살펴보겠습니다.\n",
        "다음과 같은 내용들을 알게 됩니다:\n",
        "\n",
        "   - 반복자(iterator)로 가공되지 않은 데이터(raw data)에 접근하기\n",
        "   - 가공되지 않은 텍스트 문장들을 모델 학습에 사용할 수 있는 ``torch.Tensor`` 로 변환하는 데이터 처리 파이프라인 만들기\n",
        "   - `torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__ 를 사용하여 데이터를 섞고 반복하기(shuffle and iterate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2xkUcXffrMy"
      },
      "source": [
        "기초 데이터셋 반복자(raw data iterator)에 접근하기\n",
        "-------------------------------------------------------------\n",
        "\n",
        "torchtext 라이브러리는 가공되지 않은 텍스트 문장들을 만드는(yield) 몇 가지 기초 데이터셋 반복자(raw dataset iterator)를 제공합니다.\n",
        "예를 들어, ``AG_NEWS`` 데이터셋 반복자는 레이블(label)과 문장의 튜플(tuple) 형태로 가공되지 않은 데이터를 만듭니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o_sHSRWsfrMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cedcfb0-fa3d-4f9a-a0c8-83d47c0cb7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "29.5MB [00:00, 90.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "train_iter = AG_NEWS(split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muxXmiBEfrMz"
      },
      "source": [
        "::\n",
        "\n",
        "    next(train_iter)\n",
        "    >>> (3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters -\n",
        "    Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green\n",
        "    again.\")\n",
        "\n",
        "    next(train_iter)\n",
        "    >>> (3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private\n",
        "    investment firm Carlyle Group,\\\\which has a reputation for making well-timed\n",
        "    and occasionally\\\\controversial plays in the defense industry, has quietly\n",
        "    placed\\\\its bets on another part of the market.')\n",
        "\n",
        "    next(train_iter)\n",
        "    >>> (3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring\n",
        "    crude prices plus worries\\\\about the economy and the outlook for earnings are\n",
        "    expected to\\\\hang over the stock market next week during the depth of\n",
        "    the\\\\summer doldrums.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuDSX66TfrM0"
      },
      "source": [
        "데이터 처리 파이프라인 준비하기\n",
        "---------------------------------\n",
        "\n",
        "어휘집(vocab), 단어 벡터(word vector), 토크나이저(tokenizer)를 포함하여 torchtext 라이브러리의 가장 기본적인 구성요소를 재검토했습니다.\n",
        "이들은 가공되지 않은 텍스트 문자열에 대한 기본적인 데이터 처리 빌딩 블록(data processing building block)입니다.\n",
        "\n",
        "다음은 토크나이저 및 어휘집을 사용한 일반적인 NLP 데이터 처리의 예입니다.\n",
        "첫번째 단계는 가공되지 않은 학습 데이터셋으로 어휘집을 만드는 것입니다.\n",
        "여기에서는 토큰의 목록 또는 반복자를 받는 내장(built-in) 팩토리 함수(factory function) `build_vocab_from_iterator` 를 사용합니다.\n",
        "사용자는 어휘집에 추가할 특수 기호(special symbol) 같은 것들을 전달할 수도 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3pycO0MVfrM1"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spcfbnatfrM1"
      },
      "source": [
        "어휘집 블록(vocabulary block)은 토큰 목록을 정수로 변환합니다.\n",
        "\n",
        "::\n",
        "\n",
        "    vocab(['here', 'is', 'an', 'example'])\n",
        "    >>> [475, 21, 30, 5286]\n",
        "\n",
        "토크나이저와 어휘집을 갖춘 텍스트 처리 파이프라인을 준비합니다.\n",
        "텍스트 파이프라인과 레이블(label) 파이프라인은 데이터셋 반복자로부터 얻어온 가공되지 않은 문장 데이터를 처리하기 위해 사용됩니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dcx3XTpdfrM2"
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNNn_nz-frM2"
      },
      "source": [
        "텍스트 파이프라인은 어휘집에 정의된 룩업 테이블(순람표; lookup table)에 기반하여 텍스트 문장을 정수 목록으로 변환합니다.\n",
        "레이블(label) 파이프라인은 레이블을 정수로 변환합니다. 예를 들어,\n",
        "\n",
        "::\n",
        "\n",
        "    text_pipeline('here is the an example')\n",
        "    >>> [475, 21, 2, 30, 5286]\n",
        "    label_pipeline('10')\n",
        "    >>> 9\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-I8ML4LfrM3"
      },
      "source": [
        "데이터 배치(batch)와 반복자 생성하기\n",
        "----------------------------------------\n",
        "\n",
        "`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__ 를\n",
        "권장합니다. (튜토리얼은 `여기 <https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html>`__ 있습니다.)\n",
        "이는 ``getitem()`` 과 ``len()`` 프로토콜을 구현한 맵 형태(map-style)의 데이터셋으로 동작하며, 맵(map)처럼 인덱스/키로 데이터 샘플을 얻어옵니다.\n",
        "또한, 셔플(shuffle) 인자를 ``False`` 로 설정하면 순회 가능한(iterable) 데이터셋처럼 동작합니다.\n",
        "\n",
        "모델로 보내기 전, ``collate_fn`` 함수는 ``DataLoader`` 로부터 생성된 샘플 배치로 동작합니다.\n",
        "``collate_fn`` 의 입력은 ``DataLoader`` 에 배치 크기(batch size)가 있는 배치(batch) 데이터이며,\n",
        "``collate_fn`` 은 이를 미리 선언된 데이터 처리 파이프라인에 따라 처리합니다.\n",
        "``collate_fn`` 이 최상위 수준으로 정의(top level def)되었는지 확인합니다. 이렇게 하면 모든 워커에서 이 함수를 사용할 수 있습니다.\n",
        "\n",
        "아래 예제에서, 주어진(original) 데이터 배치의 텍스트 항목들은 리스트(list)에 담긴(pack) 뒤 ``nn.EmbeddingBag`` 의 입력을 위한 하나의 tensor로 합쳐(concatenate)집니다.\n",
        "오프셋(offset)은 텍스트 tensor에서 개별 시퀀스 시작 인덱스를 표현하기 위한 구분자(delimiter) tensor입니다.\n",
        "레이블(label)은 개별 텍스트 항목의 레이블을 저장하는 tensor입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7W6twvOTfrM3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITu1pTefrM3"
      },
      "source": [
        "모델 정의하기\n",
        "---------------\n",
        "\n",
        "모델은\n",
        "`nn.EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`__\n",
        "레이어와 분류(classification) 목적을 위한 선형 레이어로 구성됩니다.\n",
        "기본 모드가 \"평균(mean)\"인 ``nn.EmbeddingBag`` 은 임베딩들의 \"가방(bag)\"의 평균 값을 계산합니다.\n",
        "이때 텍스트(text) 항목들은 각기 그 길이가 다를 수 있지만, ``nn.EmbeddingBag`` 모듈은 텍스트의 길이를\n",
        "오프셋(offset)으로 저장하고 있으므로 패딩(padding)이 필요하지는 않습니다.\n",
        "\n",
        "덧붙여서, ``nn.EmbeddingBag`` 은 임베딩의 평균을 즉시 계산하기 때문에,\n",
        "tensor들의 시퀀스를 처리할 때 성능 및 메모리 효율성 측면에서의 장점도\n",
        "갖고 있습니다.\n",
        "\n",
        "![](https://github.com/9bow/PyTorch-Tutorials-kr/blob/master/docs/_downloads/_static/img/text_sentiment_ngrams_model.png?raw=1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kPDoznTWfrM4"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC1ErO07frM4"
      },
      "source": [
        "인스턴스 생성하기\n",
        "-----------------\n",
        "\n",
        "``AG_NEWS`` 데이터셋에는 4종류의 레이블이 존재하므로 클래스의 개수도 4개입니다.\n",
        "\n",
        "::\n",
        "\n",
        "   1 : World (세계)\n",
        "   2 : Sports (스포츠)\n",
        "   3 : Business (경제)\n",
        "   4 : Sci/Tec (과학/기술)\n",
        "\n",
        "임베딩 차원이 64인 모델을 만듭니다.\n",
        "어휘집의 크기(Vocab size)는 어휘집(vocab)의 길이와 같습니다.\n",
        "클래스의 개수는 레이블의 개수와 같습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dkQKHAwjfrM4"
      },
      "outputs": [],
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcniiP1AfrM5"
      },
      "source": [
        "모델을 학습하고 결과를 평가하는 함수 정의하기\n",
        "---------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AO-3cTJ-frM5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUcL0RWmfrM5"
      },
      "source": [
        "데이터셋을 분할하고 모델 수행하기\n",
        "---------------------------------\n",
        "\n",
        "원본 ``AG_NEWS`` 에는 검증용 데이터가 포함되어 있지 않기 때문에, 우리는 학습\n",
        "데이터를 학습 및 검증 데이터로 분할하려 합니다. 이때 데이터를 분할하는\n",
        "비율은 0.95(학습)와 0.05(검증) 입니다. 우리는 여기서 PyTorch의\n",
        "핵심 라이브러리 중 하나인\n",
        "`torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>`__\n",
        "함수를 사용합니다.\n",
        "\n",
        "`CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
        "기준(criterion)은 각 클래스에 대해 ``nn.LogSoftmax()`` 와 ``nn.NLLLoss()`` 를\n",
        "합쳐놓은 방식입니다.\n",
        "`SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>`__\n",
        "optimizer는 확률적 경사 하강법를 구현해놓은 것입니다. 처음의 학습률은\n",
        "5.0으로 두었습니다. 매 에폭을 진행하면서 학습률을 조절할 때는\n",
        "`StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>`__\n",
        "을 사용합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wQSlEjyvfrM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcdec8b7-e79a-4a92-cc8b-41c067e7b744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.86MB [00:00, 33.0MB/s]                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 1782 batches | accuracy    0.695\n",
            "| epoch   1 |  1000/ 1782 batches | accuracy    0.857\n",
            "| epoch   1 |  1500/ 1782 batches | accuracy    0.880\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  7.02s | valid accuracy    0.884 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 1782 batches | accuracy    0.900\n",
            "| epoch   2 |  1000/ 1782 batches | accuracy    0.899\n",
            "| epoch   2 |  1500/ 1782 batches | accuracy    0.901\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  6.95s | valid accuracy    0.888 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 1782 batches | accuracy    0.912\n",
            "| epoch   3 |  1000/ 1782 batches | accuracy    0.915\n",
            "| epoch   3 |  1500/ 1782 batches | accuracy    0.914\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  8.57s | valid accuracy    0.904 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 1782 batches | accuracy    0.924\n",
            "| epoch   4 |  1000/ 1782 batches | accuracy    0.923\n",
            "| epoch   4 |  1500/ 1782 batches | accuracy    0.924\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  6.96s | valid accuracy    0.911 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 1782 batches | accuracy    0.932\n",
            "| epoch   5 |  1000/ 1782 batches | accuracy    0.928\n",
            "| epoch   5 |  1500/ 1782 batches | accuracy    0.928\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  7.07s | valid accuracy    0.898 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 1782 batches | accuracy    0.942\n",
            "| epoch   6 |  1000/ 1782 batches | accuracy    0.942\n",
            "| epoch   6 |  1500/ 1782 batches | accuracy    0.943\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  7.16s | valid accuracy    0.912 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 1782 batches | accuracy    0.946\n",
            "| epoch   7 |  1000/ 1782 batches | accuracy    0.944\n",
            "| epoch   7 |  1500/ 1782 batches | accuracy    0.944\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  7.32s | valid accuracy    0.913 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 1782 batches | accuracy    0.946\n",
            "| epoch   8 |  1000/ 1782 batches | accuracy    0.946\n",
            "| epoch   8 |  1500/ 1782 batches | accuracy    0.945\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time:  6.80s | valid accuracy    0.913 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 1782 batches | accuracy    0.947\n",
            "| epoch   9 |  1000/ 1782 batches | accuracy    0.945\n",
            "| epoch   9 |  1500/ 1782 batches | accuracy    0.946\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  7.37s | valid accuracy    0.913 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 1782 batches | accuracy    0.948\n",
            "| epoch  10 |  1000/ 1782 batches | accuracy    0.947\n",
            "| epoch  10 |  1500/ 1782 batches | accuracy    0.949\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  6.97s | valid accuracy    0.913 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNJI40tTfrM6"
      },
      "source": [
        "평가 데이터로 모델 평가하기\n",
        "-------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_KMBpLfrM6"
      },
      "source": [
        "평가 데이터셋을 통한 결과를 확인합니다...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LrbrdQf_frM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94946933-d5fa-4794-9c3f-2597a3a12b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.907\n"
          ]
        }
      ],
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M8-clP0frM6"
      },
      "source": [
        "임의의 뉴스로 평가하기\n",
        "----------------------\n",
        "\n",
        "현재까지 최고의 모델로 골프 뉴스를 테스트해보겠습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vuR1bdrefrM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d0dced-4091-4578-c280-fbb31887f827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a Sports news\n"
          ]
        }
      ],
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Iv0ihsF_hhVp"
      },
      "execution_count": 11,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "text_sentiment_ngrams_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}